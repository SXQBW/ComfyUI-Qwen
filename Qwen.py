from __future__ import annotations
import torch
import os
import json
import re
import time
import requests
from transformers import pipeline, TextStreamer, TextIteratorStreamer, BitsAndBytesConfig
from threading import Thread
from huggingface_hub import snapshot_download
from modelscope.hub.snapshot_download import snapshot_download as modelscope_snapshot_download
import folder_paths
from pathlib import Path

# åŒè¯­åˆ†éš”ç¬¦
BILINGUAL_SEPARATOR = " | "

def replace_thinking_tags(text):
    """å°†æ–‡æœ¬ä¸­çš„<think>å’Œ</think>æ ‡ç­¾æ›¿æ¢ä¸ºåŒè¯­æè¿°
    Replace <think> and </think> tags in the text with bilingual descriptions"""

    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    if not isinstance(text, str):
        if isinstance(text, list):
            text = " ".join(map(str, text))
        else:
            text = str(text)

    # æ›¿æ¢å¼€å§‹æ ‡ç­¾ï¼Œä½¿ç”¨ç¾è§‚çš„åˆ†éš”ç¬¦
    text = re.sub(r'<think>', 
                  f'ğŸŒ æ·±åº¦æ€è€ƒå¯åŠ¨ | Deep Thinking Start ------------------------------', 
                  text)
    # æ›¿æ¢ç»“æŸæ ‡ç­¾
    text = re.sub(r'</think>', 
                  f'âœ… æ·±åº¦æ€è€ƒå®Œæˆ | Deep Thinking Complete ----------------------------', 
                  text)
    return text

def remove_thinking_tags(text):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„æ€è€ƒå†…å®¹æ ‡ç­¾å’Œå†…å®¹
    Remove thinking tags and their content from the text"""
    # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
    clean_text = re.sub(r'ğŸŒ æ·±åº¦æ€è€ƒå¯åŠ¨.*?âœ… æ·±åº¦æ€è€ƒå®Œæˆ', 
                        '', text, flags=re.DOTALL)
    
    # é¢å¤–å¤„ç†å¯èƒ½çš„å˜ä½“ï¼Œç¡®ä¿æ‰€æœ‰æ€è€ƒæ ‡ç­¾éƒ½è¢«ç§»é™¤
    clean_text = re.sub(r'\| Deep Thinking Complete\s*----------------------------', 
                        '', clean_text)

    return clean_text.strip()

def markdown_to_plaintext(text):
    """å°†Markdownæ–‡æœ¬è½¬æ¢ä¸ºçº¯æ–‡æœ¬
    Convert Markdown text to plain text"""
    # ç§»é™¤æ ‡é¢˜æ ‡è®°
    text = re.sub(r'^(#+)\s*(.*)$', r'\2', text, flags=re.MULTILINE)
    
    # ç§»é™¤ç²—ä½“/æ–œä½“æ ‡è®°
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    text = re.sub(r'\*(.*?)\*', r'\1', text)
    
    # ç§»é™¤åˆ—è¡¨æ ‡è®°
    text = re.sub(r'^[\s]*[-*+]\s+(.*)$', r'\1', text, flags=re.MULTILINE)
    
    # ç§»é™¤é“¾æ¥æ ‡è®°
    text = re.sub(r'\[(.*?)\]\((.*?)\)', r'\1', text)
    
    return text

# æ¨¡å‹æ³¨å†Œè¡¨JSONæ–‡ä»¶è·¯å¾„ - ä¿æŒåœ¨åŸç›®å½•ï¼Œä¸ç§»åŠ¨åˆ°Qwen/Qwenç›®å½•
MODEL_REGISTRY_JSON = os.path.join(os.path.dirname(os.path.abspath(__file__)), "qwen_model_registry.json")

def load_model_registry():
    """ä»JSONæ–‡ä»¶åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨
    Load model registry from JSON file"""
    try:
        if os.path.exists(MODEL_REGISTRY_JSON):
            with open(MODEL_REGISTRY_JSON, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„æ³¨å†Œè¡¨
            print(f"æ¨¡å‹æ³¨å†Œè¡¨æ–‡ä»¶ {MODEL_REGISTRY_JSON} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç©ºæ³¨å†Œè¡¨ | "
                  f"Model registry file {MODEL_REGISTRY_JSON} does not exist, using empty registry")
            return {}
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: è§£ææ¨¡å‹æ³¨å†Œè¡¨JSONæ–‡ä»¶æ—¶å‡ºé”™: {e} | "
              f"Error: Failed to parse model registry JSON file: {e}")
        return {}

# åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨
MODEL_REGISTRY = load_model_registry()

def get_gpu_info():
    """è·å–GPUä¿¡æ¯ï¼ŒåŒ…æ‹¬æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    Get GPU information, including memory usage"""
    try:
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            device = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(device)
            total_memory = props.total_memory / 1024**3  # GB
            allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # GB
            free_memory = total_memory - allocated_memory
            
            return {
                "available": True,
                "count": gpu_count,
                "name": props.name,
                "total_memory": total_memory,
                "allocated_memory": allocated_memory,
                "free_memory": free_memory
            }
        else:
            return {
                "available": False,
                "count": 0,
                "name": "None",
                "total_memory": 0,
                "allocated_memory": 0,
                "free_memory": 0
            }
    except Exception as e:
        print(f"è·å–GPUä¿¡æ¯æ—¶å‡ºé”™: {e} | "
              f"Error getting GPU information: {e}")
        return {
            "available": False,
            "count": 0,
            "name": "None",
            "total_memory": 0,
            "allocated_memory": 0,
            "free_memory": 0
        }

def get_system_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ€»å†…å­˜å’Œå¯ç”¨å†…å­˜
    Get system memory information, including total and available memory"""
    try:
        import psutil
        mem = psutil.virtual_memory()
        return {
            "total": mem.total / 1024**3,  # GB
            "available": mem.available / 1024**3,  # GB
            "used": mem.used / 1024**3,  # GB
            "percent": mem.percent
        }
    except ImportError:
        print("è­¦å‘Š: æ— æ³•å¯¼å…¥psutilåº“ï¼Œç³»ç»Ÿå†…å­˜æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨ | "
              "Warning: Failed to import psutil library, system memory detection disabled")
        return {
            "total": 0,
            "available": 0,
            "used": 0,
            "percent": 0
        }

def get_device_info():
    """è·å–è®¾å¤‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬GPUå’ŒCPUï¼Œå¹¶åˆ†ææœ€ä½³è¿è¡Œè®¾å¤‡
    Get device information, including GPU and CPU, and analyze optimal running device"""
    device_info = {
        "device_type": "unknown",
        "gpu": get_gpu_info(),
        "system_memory": get_system_memory_info(),
        "recommended_device": "cpu",  # é»˜è®¤æ¨èCPU | Default to CPU
        "memory_sufficient": True,
        "warning_message": None
    }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºApple Silicon
    try:
        import platform
        if platform.system() == "Darwin" and platform.processor() == "arm":
            device_info["device_type"] = "apple_silicon"
            # M1/M2èŠ¯ç‰‡æœ‰ç»Ÿä¸€å†…å­˜ï¼Œæ£€æŸ¥æ€»å†…å­˜æ˜¯å¦å……è¶³
            if device_info["system_memory"]["total"] >= 16:  # è‡³å°‘16GBå†…å­˜ | At least 16GB RAM
                device_info["recommended_device"] = "mps"
            else:
                device_info["memory_sufficient"] = False
                device_info["warning_message"] = "Apple SiliconèŠ¯ç‰‡å†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨è‡³å°‘16GBå†…å­˜çš„è®¾å¤‡ | Insufficient memory on Apple Silicon, recommend at least 16GB RAM"

            return device_info
    except:
        pass
    
    # æ£€æŸ¥æ˜¯å¦æœ‰NVIDIA GPU
    if device_info["gpu"]["available"]:
        device_info["device_type"] = "nvidia_gpu"
        # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦å……è¶³
        if device_info["gpu"]["total_memory"] >= 8:  # è‡³å°‘8GBæ˜¾å­˜ | At least 8GB VRAM
            device_info["recommended_device"] = "cuda"
        else:
            # æ˜¾å­˜ä¸è¶³ï¼Œä½†ä»å¯ä½¿ç”¨ï¼Œåªæ˜¯æ€§èƒ½ä¼šå—å½±å“
            device_info["memory_sufficient"] = False
            device_info["warning_message"] = "NVIDIA GPUæ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½ä¼šä½¿ç”¨ç³»ç»Ÿå†…å­˜ï¼Œæ€§èƒ½ä¼šä¸‹é™ | Insufficient NVIDIA GPU memory, may use system RAM with reduced performance"
            device_info["recommended_device"] = "cuda"  # ä»æ¨èä½¿ç”¨GPUï¼Œä½†ä¼šå¯ç”¨å†…å­˜ä¼˜åŒ– | Still recommend GPU with memory optimization
        return device_info
    
    # æ£€æŸ¥æ˜¯å¦æœ‰AMD GPU (ROCm)
    try:
        import torch
        if hasattr(torch, 'device') and torch.device('cuda' if torch.cuda.is_available() else 'cpu').type == 'cuda':
            device_info["device_type"] = "amd_gpu"
            # AMD GPUå†…å­˜æ£€æŸ¥
            if device_info["gpu"]["total_memory"] >= 8:
                device_info["recommended_device"] = "cuda"
            else:
                device_info["memory_sufficient"] = False
                device_info["warning_message"] = "AMD GPUæ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½ä¼šä½¿ç”¨ç³»ç»Ÿå†…å­˜ï¼Œæ€§èƒ½ä¼šä¸‹é™ | Insufficient AMD GPU memory, may use system RAM with reduced performance"

                device_info["recommended_device"] = "cuda"
            return device_info
    except:
        pass
    
    # é»˜è®¤ä¸ºCPU
    device_info["device_type"] = "cpu"
    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜æ˜¯å¦å……è¶³
    if device_info["system_memory"]["total"] < 8:
        device_info["memory_sufficient"] = False
        device_info["warning_message"] = "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ¨¡å‹è¿è¡Œå¯èƒ½ä¼šéå¸¸ç¼“æ…¢ | Insufficient system memory, model may run very slowly"
    return device_info

def calculate_required_memory(model_name, quantization, use_cpu=False, use_mps=False):
    """æ ¹æ®æ¨¡å‹åç§°ã€é‡åŒ–æ–¹å¼å’Œè®¾å¤‡ç±»å‹è®¡ç®—æ‰€éœ€å†…å­˜
    Calculate required memory based on model name, quantization, and device type"""
    model_info = MODEL_REGISTRY.get(model_name, {})
    vram_config = model_info.get("vram_requirement", {})
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»é‡åŒ–
    is_quantized_model = model_info.get("quantized", False)
    
    # åŸºç¡€å†…å­˜éœ€æ±‚è®¡ç®—
    if is_quantized_model:
        base_memory = vram_config.get("full", 0)
    else:
        if quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
            base_memory = vram_config.get("4bit", 0)
        elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
            base_memory = vram_config.get("8bit", 0)
        else:
            base_memory = vram_config.get("full", 0)
    
    # è°ƒæ•´å†…å­˜éœ€æ±‚ï¼ˆCPUå’ŒMPSé€šå¸¸éœ€è¦æ›´å¤šå†…å­˜ï¼‰
    if use_cpu or use_mps:
        # CPUå’ŒMPSé€šå¸¸éœ€è¦æ›´å¤šå†…å­˜ç”¨äºå†…å­˜äº¤æ¢
        memory_factor = 1.5 if use_cpu else 1.2
        return base_memory * memory_factor
    
    return base_memory

def check_flash_attention():
    """æ£€æµ‹Flash Attention 2æ”¯æŒï¼ˆéœ€Ampereæ¶æ„åŠä»¥ä¸Šï¼‰
    Check for Flash Attention 2 support (requires Ampere architecture or higher)"""
    try:
        from flash_attn import flash_attn_func
        major, _ = torch.cuda.get_device_capability()
        return major >= 8  # ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›8.0+çš„GPU | Only support GPUs with compute capability 8.0+
    except ImportError:
        return False

FLASH_ATTENTION_AVAILABLE = check_flash_attention()

def init_qwen_paths(model_name):
    """åˆå§‹åŒ–æ¨¡å‹è·¯å¾„ï¼Œæ”¯æŒåŠ¨æ€ç”Ÿæˆä¸åŒæ¨¡å‹ç‰ˆæœ¬çš„è·¯å¾„
    Initialize model paths, supporting dynamic generation of paths for different model versions"""
    base_dir = Path(folder_paths.models_dir).resolve()
    qwen_dir = base_dir / "Qwen" / "Qwen"  # æ·»åŠ Qwenå­ç›®å½• | Add Qwen subdirectory
    model_dir = qwen_dir / model_name  # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºå­ç›®å½• | Use model name as subdirectory
    
    # åˆ›å»ºç›®å½•
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ³¨å†Œåˆ°ComfyUI
    if hasattr(folder_paths, "add_model_folder_path"):
        folder_paths.add_model_folder_path("Qwen", str(qwen_dir))
    else:
        folder_paths.folder_names_and_paths["Qwen"] = ([str(qwen_dir)], {'.safetensors', '.bin', '.gguf' })
    
    print(f"æ¨¡å‹è·¯å¾„å·²åˆå§‹åŒ–: {model_dir} | "
          f"Model path initialized: {model_dir}")
    return str(model_dir)  # ä¿®æ”¹ï¼šè¿”å›æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œè€Œä¸æ˜¯çˆ¶ç›®å½• | Return model directory path instead of parent directory

def test_download_speed(url):
    """æµ‹è¯•ä¸‹è½½é€Ÿåº¦ï¼Œä¸‹è½½ 5 ç§’
    Test download speed by downloading for 5 seconds"""
    try:
        start_time = time.time()
        response = requests.get(url, stream=True, timeout=10)
        downloaded_size = 0
        for data in response.iter_content(chunk_size=1024):
            if time.time() - start_time > 5:
                break
            downloaded_size += len(data)
        end_time = time.time()
        speed = downloaded_size / (end_time - start_time) / 1024  # KB/s
        return speed
    except Exception as e:
        print(f"æµ‹è¯•ä¸‹è½½é€Ÿåº¦æ—¶å‡ºç°é”™è¯¯: {e} | "
              f"Error testing download speed: {e}")
        return 0

def validate_model_path(model_path, model_name):
    """éªŒè¯æ¨¡å‹è·¯å¾„çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨
    Validate the validity of the model path and check if model files are complete"""
    path_obj = Path(model_path)
    
    # åŸºæœ¬è·¯å¾„æ£€æŸ¥
    if not path_obj.is_absolute():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç»å¯¹è·¯å¾„ | "
              f"Error: {model_path} is not an absolute path")
        return False
    
    if not path_obj.exists():
        print(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path} | "
              f"Model directory does not exist: {model_path}")
        return False
    
    if not path_obj.is_dir():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç›®å½• | "
              f"Error: {model_path} is not a directory")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨
    if not check_model_files_exist(model_path, model_name):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: {model_path} | "
              f"Model files incomplete: {model_path}")
        return False
    
    return True

def check_model_files_exist(model_dir, model_name):
    """æ£€æŸ¥ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬æ‰€éœ€çš„æ–‡ä»¶æ˜¯å¦é½å…¨
    Check if required files for a specific model version are complete"""
    if model_name not in MODEL_REGISTRY:
        print(f"é”™è¯¯: æœªçŸ¥æ¨¡å‹ç‰ˆæœ¬ {model_name} | "
              f"Error: Unknown model version {model_name}")
        return False
    
    required_files = MODEL_REGISTRY[model_name]["required_files"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_dir, file)):
            return False
    return True

class QwenTextProcessor:
    def __init__(self):
        # é»˜è®¤ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„ç¬¬ä¸€ä¸ªé»˜è®¤æ¨¡å‹
        default_model = next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                            list(MODEL_REGISTRY.keys())[0])       

        # é‡ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…å¹²æ‰°
        os.environ.pop("HUGGINGFACE_HUB_CACHE", None)   
        self.current_model_name = default_model
        self.current_quantization = None  # è®°å½•å½“å‰çš„é‡åŒ–é…ç½®
        self.model_path = init_qwen_paths(self.current_model_name)
        self.cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path} | "
              f"Model path: {self.model_path}")
        print(f"ç¼“å­˜è·¯å¾„: {self.cache_dir} | "
              f"Cache path: {self.cache_dir}")

        # éªŒè¯å¹¶åˆ›å»ºç¼“å­˜ç›®å½•
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

        self.model = None
        self.tokenizer = None
        # æ€§èƒ½ç»Ÿè®¡
        self.generation_stats = {"count": 0, "total_time": 0}        
     
        # åˆå§‹åŒ–è®¾å¤‡ä¿¡æ¯
        self.device_info = get_device_info()
        self.default_device = self.device_info["recommended_device"]
        
        print(f"æ£€æµ‹åˆ°çš„è®¾å¤‡: {self.device_info['device_type']} | "
              f"Detected device: {self.device_info['device_type']}")
        print(f"è‡ªåŠ¨é€‰æ‹©çš„è¿è¡Œè®¾å¤‡: {self.default_device} | "
              f"Automatically selected device: {self.default_device}")
        
        if not self.device_info["memory_sufficient"]:
            print(f"è­¦å‘Š: {self.device_info['warning_message']}")
        
        # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–é€‰é¡¹
        self.optimize_for_low_memory = not self.device_info["memory_sufficient"]
        
    def clear_model_resources(self):
        """é‡Šæ”¾å½“å‰æ¨¡å‹å ç”¨çš„èµ„æº
        Release resources occupied by the current model"""
        if self.model is not None:
            print("é‡Šæ”¾å½“å‰æ¨¡å‹å ç”¨çš„èµ„æº... | "
                  "Releasing resources occupied by current model...")
            del self.model, self.tokenizer
            self.model = None
            self.tokenizer = None
            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜ | Clear GPU cache

    def check_memory_requirements(self, model_name, quantization):
        """æ£€æŸ¥å½“å‰è®¾å¤‡å†…å­˜æ˜¯å¦æ»¡è¶³æ¨¡å‹è¦æ±‚ï¼Œå¿…è¦æ—¶è°ƒæ•´é‡åŒ–çº§åˆ«
        Check if current device memory meets model requirements, adjust quantization level if necessary"""
        # ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©çš„è®¾å¤‡
        device = self.default_device
        use_cpu = device == "cpu"
        use_mps = device == "mps"
        
        # è®¡ç®—æ‰€éœ€å†…å­˜
        required_memory = calculate_required_memory(model_name, quantization, use_cpu, use_mps)
        
        if use_cpu or use_mps:
            # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
            available_memory = self.device_info["system_memory"]["available"]
            memory_type = "ç³»ç»Ÿå†…å­˜ | System memory"
        else:
            # æ£€æŸ¥GPUå†…å­˜
            available_memory = self.device_info["gpu"]["free_memory"]
            memory_type = "GPUæ˜¾å­˜ | GPU memory"
        
        # æ·»åŠ 20%çš„å®‰å…¨ä½™é‡
        safety_margin = 1.2
        required_memory_with_margin = required_memory * safety_margin
        
        print(f"æ¨¡å‹ {model_name} (é‡åŒ–: {quantization}) éœ€è¦ {required_memory:.2f} GB {memory_type} | "
              f"Model {model_name} (quantization: {quantization}) requires {required_memory:.2f} GB {memory_type}")
        print(f"è€ƒè™‘å®‰å…¨ä½™é‡åï¼Œéœ€è¦ {required_memory_with_margin:.2f} GB {memory_type} | "
              f"With safety margin, requires {required_memory_with_margin:.2f} GB {memory_type}")
        print(f"å½“å‰å¯ç”¨ {memory_type}: {available_memory:.2f} GB | "
              f"Current available {memory_type}: {available_memory:.2f} GB")
        
        # å¦‚æœå†…å­˜ä¸è¶³ï¼Œè‡ªåŠ¨è°ƒæ•´é‡åŒ–çº§åˆ«
        if required_memory_with_margin > available_memory:
            print(f"è­¦å‘Š: æ‰€é€‰é‡åŒ–çº§åˆ«éœ€è¦çš„{memory_type}è¶…è¿‡å¯ç”¨å†…å­˜ï¼Œè‡ªåŠ¨è°ƒæ•´é‡åŒ–çº§åˆ« | "
                  f"Warning: Selected quantization level requires more {memory_type} than available, automatically adjusting")
            
            # é™çº§ç­–ç•¥
            if quantization == "ğŸš« None (Original Precision)":
                print("å°†é‡åŒ–çº§åˆ«ä»'æ— é‡åŒ–'è°ƒæ•´ä¸º'8-bit' | "
                      "Adjusting quantization from 'None' to '8-bit'")
                return "âš–ï¸ 8-bit (Balanced Precision)"
            elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
                print("å°†é‡åŒ–çº§åˆ«ä»'8-bit'è°ƒæ•´ä¸º'4-bit' | "
                      "Adjusting quantization from '8-bit' to '4-bit'")
                return "ğŸ‘ 4-bit (VRAM-friendly)"
            else:
                # å·²ç»æ˜¯4-bitï¼Œæ— æ³•å†é™çº§
                print(f"é”™è¯¯: å³ä½¿ä½¿ç”¨4-bité‡åŒ–ï¼Œæ¨¡å‹ä»ç„¶éœ€è¦æ›´å¤š{memory_type} | "
                      f"Error: Even with 4-bit quantization, model requires more {memory_type}")
                raise RuntimeError(f"é”™è¯¯: å¯ç”¨{memory_type}ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {required_memory_with_margin:.2f} GBï¼Œä½†åªæœ‰ {available_memory:.2f} GB | "
                                   f"Error: Insufficient {memory_type}, requires at least {required_memory_with_margin:.2f} GB, but only {available_memory:.2f} GB available")
        
        return quantization

    def load_model(self, model_name, quantization, enable_thinking=True):
        """åŠ è½½æŒ‡å®šæ¨¡å‹å’Œé‡åŒ–é…ç½®ï¼Œæ”¯æŒæ€è€ƒæ¨¡å¼
        Load specified model and quantization configuration, supporting thinking mode"""
        # æ£€æŸ¥å†…å­˜éœ€æ±‚å¹¶å¯èƒ½è°ƒæ•´é‡åŒ–çº§åˆ«
        adjusted_quantization = self.check_memory_requirements(model_name, quantization)
        
        # ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©çš„è®¾å¤‡
        device = self.default_device
        print(f"ä½¿ç”¨è®¾å¤‡: {device} | "
              f"Using device: {device}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.current_quantization == adjusted_quantization):
            print(f"ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹: {model_name}ï¼Œé‡åŒ–: {adjusted_quantization} | "
                  f"Using already loaded model: {model_name}, quantization: {adjusted_quantization}")
            return
        
        # éœ€è¦é‡æ–°åŠ è½½ï¼Œå…ˆé‡Šæ”¾ç°æœ‰èµ„æº
        self.clear_model_resources()
        
        # æ›´æ–°å½“å‰æ¨¡å‹åç§°å’Œé‡åŒ–é…ç½®
        self.current_model_name = model_name
        self.model_path = init_qwen_paths(self.current_model_name)  # ä¿®æ”¹ï¼šè·å–æ¨¡å‹ç›®å½•è·¯å¾„
        self.current_quantization = adjusted_quantization

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
        if not validate_model_path(self.model_path, self.current_model_name):
            print(f"æ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨ä¸ºä½ ä¸‹è½½ {model_name} æ¨¡å‹ï¼Œè¯·ç¨å€™... | "
                  f"Model files detected missing, downloading {model_name} model, please wait...")
            print(f"ä¸‹è½½å°†ä¿å­˜åœ¨: {self.model_path} | "
                  f"Download will be saved to: {self.model_path}")
            
            # å¼€å§‹ä¸‹è½½é€»è¾‘
            try:
                # ä»æ³¨å†Œè¡¨è·å–æ¨¡å‹ä¿¡æ¯
                model_info = MODEL_REGISTRY[model_name]
                
                # æµ‹è¯•ä¸‹è½½é€Ÿåº¦
                huggingface_test_url = f"https://huggingface.co/{model_info['repo_id']['huggingface']}/resolve/main/{model_info['test_file']}"
                modelscope_test_url = f"https://modelscope.cn/api/v1/models/{model_info['repo_id']['modelscope']}/repo?Revision=master&FilePath={model_info['test_file']}"
                huggingface_speed = test_download_speed(huggingface_test_url)
                modelscope_speed = test_download_speed(modelscope_test_url)

                print(f"Hugging Faceä¸‹è½½é€Ÿåº¦: {huggingface_speed:.2f} KB/s | "
                      f"Hugging Face download speed: {huggingface_speed:.2f} KB/s")
                print(f"ModelScopeä¸‹è½½é€Ÿåº¦: {modelscope_speed:.2f} KB/s | "
                      f"ModelScope download speed: {modelscope_speed:.2f} KB/s")

                # æ ¹æ®ä¸‹è½½é€Ÿåº¦é€‰æ‹©ä¼˜å…ˆä¸‹è½½æº
                if huggingface_speed > modelscope_speed * 1.5:
                    download_sources = [
                        (snapshot_download, model_info['repo_id']['huggingface'], "Hugging Face"),
                        (modelscope_snapshot_download, model_info['repo_id']['modelscope'], "ModelScope")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»Hugging Faceä¸‹è½½ | "
                          "Based on download speed analysis, attempting download from Hugging Face first")
                else:
                    download_sources = [
                        (modelscope_snapshot_download, model_info['repo_id']['modelscope'], "ModelScope"),
                        (snapshot_download, model_info['repo_id']['huggingface'], "Hugging Face")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»ModelScopeä¸‹è½½ | "
                          "Based on download speed analysis, attempting download from ModelScope first")

                max_retries = 3
                success = False
                final_error = None
                used_cache_path = None

                for download_func, repo_id, source in download_sources:
                    for retry in range(max_retries):
                        try:
                            print(f"å¼€å§‹ä» {source} ä¸‹è½½æ¨¡å‹ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰... | "
                                  f"Starting model download from {source} (attempt {retry + 1})...")
                            if download_func == snapshot_download:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    ignore_patterns=["*.msgpack", "*.h5"],
                                    resume_download=True,
                                    local_files_only=False
                                )
                            else:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    revision="master"
                                )

                            used_cache_path = cached_path  # è®°å½•ä½¿ç”¨çš„ç¼“å­˜è·¯å¾„
                            
                            # å°†ä¸‹è½½çš„æ¨¡å‹å¤åˆ¶åˆ°æ¨¡å‹ç›®å½•
                            self.copy_cached_model_to_local(cached_path, self.model_path)
                            
                            print(f"æˆåŠŸä» {source} ä¸‹è½½æ¨¡å‹åˆ° {self.model_path} | "
                                  f"Successfully downloaded model from {source} to {self.model_path}")
                            success = True
                            break

                        except Exception as e:
                            final_error = e  # ä¿å­˜æœ€åä¸€ä¸ªé”™è¯¯
                            if retry < max_retries - 1:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå³å°†è¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•... | "
                                      f"Failed to download model from {source} (attempt {retry + 1}): {e}, trying again...")
                            else:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå°è¯•å…¶ä»–æº... | "
                                      f"Failed to download model from {source} (attempt {retry + 1}): {e}, trying next source...")
                    if success:
                        break
                else:
                    raise RuntimeError("ä»æ‰€æœ‰æºä¸‹è½½æ¨¡å‹å‡å¤±è´¥ã€‚ | "
                                      "Failed to download model from all sources.")
                
                # ä¸‹è½½å®Œæˆåå†æ¬¡éªŒè¯
                if not validate_model_path(self.model_path, self.current_model_name):
                    raise RuntimeError(f"ä¸‹è½½åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {self.model_path} | "
                                      f"Model files still incomplete after download: {self.model_path}")
                
                print(f"æ¨¡å‹ {model_name} å·²å‡†å¤‡å°±ç»ª | "
                      f"Model {model_name} is ready")
                
            except Exception as e:
                print(f"ä¸‹è½½æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e} | "
                      f"Error downloading model: {e}")
                
                # ä¸‹è½½å¤±è´¥æç¤º
                if used_cache_path:
                    print("\nâš ï¸ æ³¨æ„ï¼šä¸‹è½½è¿‡ç¨‹ä¸­åˆ›å»ºäº†ç¼“å­˜æ–‡ä»¶ | "
                          "\nâš ï¸ Attention: Cache files were created during download")
                    print(f"ç¼“å­˜è·¯å¾„: {used_cache_path} | "
                          f"Cache path: {used_cache_path}")
                    print("ä½ å¯ä»¥å‰å¾€æ­¤è·¯å¾„åˆ é™¤ç¼“å­˜æ–‡ä»¶ä»¥é‡Šæ”¾ç¡¬ç›˜ç©ºé—´ | "
                          "You can delete these files to free up disk space")
                
                raise RuntimeError(f"æ— æ³•ä¸‹è½½æ¨¡å‹ {model_name}ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ° {self.model_path} | "
                                  f"Unable to download model {model_name}, please download manually and place in {self.model_path}")

        # æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œæ­£å¸¸åŠ è½½
        print(f"åŠ è½½æ¨¡å‹: {self.model_path}ï¼Œé‡åŒ–: {quantization} | "
              f"Loading model: {self.model_path}, quantization: {quantization}")

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»é‡åŒ–
        is_quantized_model = MODEL_REGISTRY.get(model_name, {}).get("quantized", False)

        # å¤„ç† FP8 è·¨ GPU é—®é¢˜
        if "FP8" in model_name:
            os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

        print(f"åŠ è½½æ¨¡å‹: {model_name}ï¼Œé‡åŒ–: {adjusted_quantization}ï¼Œæ€è€ƒæ¨¡å¼: {'å¯ç”¨' if enable_thinking else 'ç¦ç”¨'} | "
              f"Loading model: {model_name}, quantization: {adjusted_quantization}, thinking mode: {'enabled' if enable_thinking else 'disabled'}")

        # é…ç½®é‡åŒ–å‚æ•°
        if is_quantized_model:
            print(f"æ¨¡å‹ {model_name} å·²ç»æ˜¯é‡åŒ–æ¨¡å‹ï¼Œå°†å¿½ç•¥ç”¨æˆ·çš„é‡åŒ–è®¾ç½® | "
                  f"Model {model_name} is already quantized, ignoring user quantization settings")
            # å¯¹äºå·²ç»é‡åŒ–çš„æ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹ç²¾åº¦åŠ è½½
            load_dtype = torch.float16
            quant_config = None
        else:
            # é…ç½®é‡åŒ–å‚æ•°
            if adjusted_quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
                load_dtype = torch.float16  # è®©é‡åŒ–é…ç½®å†³å®šæ•°æ®ç±»å‹
            elif adjusted_quantization == "âš–ï¸ 8-bit (Balanced Precision)":
                quant_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                )
                load_dtype = torch.float16  # è®©é‡åŒ–é…ç½®å†³å®šæ•°æ®ç±»å‹
            else:
                # ä¸ä½¿ç”¨é‡åŒ–ï¼Œä½¿ç”¨åŸå§‹ç²¾åº¦
                load_dtype = torch.float16
                quant_config = None

        # é…ç½®device_map
        if device == "cuda":
            if torch.cuda.device_count() > 0:
                device_map = {"": 0}  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
                print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)} | "
                      f"Using GPU: {torch.cuda.get_device_name(0)}")
            else:
                device_map = "auto"
                print("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†å°è¯•ä½¿ç”¨autoè®¾å¤‡æ˜ å°„ | "
                      "No GPU detected, attempting to use auto device mapping")
        elif device == "mps":
            device_map = "auto"  # MPSä¸æ”¯æŒdevice_mapï¼ŒåŠ è½½åéœ€æ‰‹åŠ¨ç§»åˆ°è®¾å¤‡
        else:
            device_map = "auto"  # CPUåŠ è½½

        # å‡†å¤‡åŠ è½½å‚æ•°
        load_kwargs = {
            "device_map": device_map,
            "torch_dtype": load_dtype,

        }

        # å¦‚æœæœ‰é‡åŒ–é…ç½®ï¼Œæ·»åŠ åˆ°åŠ è½½å‚æ•°ä¸­
        if quant_config is not None:
            load_kwargs["quantization_config"] = quant_config

        # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆpipeline - ä½¿ç”¨self.model_pathè€Œä¸æ˜¯model_name
        self.model = pipeline(
            "text-generation",
            model=self.model_path,  # ä½¿ç”¨æœ¬åœ°è·¯å¾„è€Œä¸æ˜¯æ¨¡å‹åç§°
            **load_kwargs
        )
        
        # è·å–tokenizer
        self.tokenizer = self.model.tokenizer

    def copy_cached_model_to_local(self, cached_path, target_path):
        """å°†ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°ç›®æ ‡è·¯å¾„
        Copy cached model files to target path"""
        print(f"æ­£åœ¨å°†æ¨¡å‹ä»ç¼“å­˜å¤åˆ¶åˆ°: {target_path} | "
              f"Copying model from cache to: {target_path}")
        target_path = Path(target_path)
        target_path.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨shutilè¿›è¡Œé€’å½’å¤åˆ¶
        import shutil
        for item in Path(cached_path).iterdir():
            if item.is_dir():
                shutil.copytree(item, target_path / item.name, dirs_exist_ok=True)
            else:
                shutil.copy2(item, target_path / item.name)
        
        # éªŒè¯å¤åˆ¶æ˜¯å¦æˆåŠŸ
        if validate_model_path(target_path, self.current_model_name):
            print(f"æ¨¡å‹å·²æˆåŠŸå¤åˆ¶åˆ° {target_path} | "
                  f"Model successfully copied to {target_path}")
        else:
            raise RuntimeError(f"å¤åˆ¶åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {target_path} | "
                              f"Model files still incomplete after copying: {target_path}")

    @torch.no_grad()
    def generate_text(self, model_name, quantization, prompt, max_tokens,  
                    messages=None, enable_thinking=False, unload_after_generation=True):
        """ç”Ÿæˆæ–‡æœ¬ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ã€æµå¼è¾“å‡ºå’Œæ€è€ƒæ¨¡å¼
        Generate text, supporting multi-turn conversations, streaming output, and thinking mode
        
        Args:
            unload_after_generation: ç”Ÿæˆåæ˜¯å¦å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾èµ„æº
        """
        start_time = time.time()
        
        # åªåœ¨å¿…è¦æ—¶åŠ è½½æ¨¡å‹ï¼ˆæ¨¡å‹åç§°æˆ–é‡åŒ–æ–¹å¼æ”¹å˜æ—¶ï¼‰
        if (self.model is None or 
            self.current_model_name != model_name or 
            self.current_quantization != quantization):
            self.load_model(model_name, quantization)  # æ³¨æ„è¿™é‡Œä¸å†ä¼ é€’enable_thinking
        
        # æ„å»ºè¾“å…¥
        if messages is not None:
            # å¤šè½®å¯¹è¯æ¨¡å¼
            input_data = messages
        else:
            # å•è½®ç”Ÿæˆæ¨¡å¼
            input_data = [{"role": "user", "content": prompt}]
        
        # æ ¹æ®æ€è€ƒæ¨¡å¼è®¾ç½®ä¸åŒçš„é‡‡æ ·å‚æ•°
        if enable_thinking:
            # æ€è€ƒæ¨¡å¼æ¨èå‚æ•°
            temperature = 0.6 
            top_p = 0.95 
        else:
            # éæ€è€ƒæ¨¡å¼æ¨èå‚æ•°
            temperature = 0.7 
            top_p = 0.8 
        
        # å‡†å¤‡ç”Ÿæˆå‚æ•°
        generate_kwargs = {
            "max_new_tokens": max(max_tokens, 10),
            "temperature": temperature,
            "do_sample": True,
            "top_p": top_p,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
        }
        
        # åº”ç”¨æ€è€ƒæ¨¡å¼ï¼ˆä½¿ç”¨å®˜æ–¹æ–¹æ³•ï¼‰
        if hasattr(self.tokenizer, 'apply_chat_template'):
            try:
                # ä½¿ç”¨å®˜æ–¹æä¾›çš„æ¨¡æ¿æ–¹æ³•ï¼Œæ˜ç¡®æ§åˆ¶æ€è€ƒæ¨¡å¼
                input_text = self.tokenizer.apply_chat_template(
                    input_data,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=enable_thinking  # ç›´æ¥æ§åˆ¶æ¨¡å‹æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
                )
                
                # ç¡®ä¿input_textæ˜¯å­—ç¬¦ä¸²ç±»å‹
                if isinstance(input_text, list):
                    input_text = " ".join(map(str, input_text))
                    
                input_data = [{"role": "user", "content": input_text}]
            except Exception as e:
                print(f"åº”ç”¨èŠå¤©æ¨¡æ¿æ—¶å‡ºé”™: {e}")
                # å¦‚æœå‡ºé”™ï¼Œå›é€€åˆ°åŸå§‹è¾“å…¥
                input_data = [{"role": "user", "content": prompt}]
        else:
            # å¦‚æœæ²¡æœ‰apply_chat_templateæ–¹æ³•ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥
            input_data = [{"role": "user", "content": prompt}]
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            pre_forward_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"ç”Ÿæˆå‰GPUå†…å­˜ä½¿ç”¨: {pre_forward_memory:.2f} MB | "
                f"GPU memory usage before generation: {pre_forward_memory:.2f} MB")
        
        # éæµå¼è¾“å‡º
        invalid_kwargs = ['low_cpu_mem_usage', 'use_safetensors']
        for key in invalid_kwargs:
            if key in generate_kwargs:
                del generate_kwargs[key]
        
        # ç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®
        if isinstance(input_data, list) and all(isinstance(item, dict) for item in input_data):
            result = self.model(input_data, **generate_kwargs)
        else:
            # å¦‚æœè¾“å…¥æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼
            input_str = input_data[0]["content"] if isinstance(input_data, list) else str(input_data)
            result = self.model(input_str, **generate_kwargs)
        
        generated_text = result[0]["generated_text"]

        # è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            post_forward_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"ç”ŸæˆåGPUå†…å­˜ä½¿ç”¨: {post_forward_memory:.2f} MB | "
                f"GPU memory usage after generation: {post_forward_memory:.2f} MB")
            print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­GPUå†…å­˜å¢åŠ : {post_forward_memory - pre_forward_memory:.2f} MB | "
                f"GPU memory increase during generation: {post_forward_memory - pre_forward_memory:.2f} MB")
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        process_time = time.time() - start_time
        self.generation_stats["count"] += 1
        self.generation_stats["total_time"] += process_time
        
        # æ‰“å°æ€§èƒ½ç»Ÿè®¡
        print(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {process_time:.2f} ç§’ | "
            f"Generation completed, time taken: {process_time:.2f} seconds")
        if self.generation_stats["count"] > 0:
            avg_time = self.generation_stats["total_time"] / self.generation_stats["count"]
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f} ç§’/æ¬¡ | "
                f"Average generation time: {avg_time:.2f} seconds per generation")
        
        # å¦‚æœå¯ç”¨äº†å¸è½½é€‰é¡¹ï¼Œé‡Šæ”¾æ¨¡å‹èµ„æº
        if unload_after_generation:
            self.clear_model_resources()
            print("æ¨¡å‹å·²å¸è½½ä»¥é‡Šæ”¾èµ„æºã€‚ä¸‹æ¬¡ä½¿ç”¨æ—¶å°†é‡æ–°åŠ è½½ã€‚ | "
                  "Model unloaded to free up resources. Will reload on next use.")
        
        # ç›´æ¥è¿”å›ç”Ÿæˆçš„å†…å®¹ï¼Œä¸å†æ‰‹åŠ¨è¿‡æ»¤æ€è€ƒå†…å®¹
        return generated_text


class QwenMultiTurnConversation:
    # ç±»çº§åˆ«å˜é‡ï¼Œç”¨äºç¼“å­˜å¤„ç†å™¨å®ä¾‹
    processor = None

    def __init__(self):
        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.chat_history = []
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (
                   list(MODEL_REGISTRY.keys()),  # åŠ¨æ€ç”Ÿæˆæ¨¡å‹é€‰é¡¹ | Dynamically generate model options
                    {
                        "default": next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                                       list(MODEL_REGISTRY.keys())[0]),
                        "tooltip": "é€‰æ‹©å¯ç”¨çš„æ¨¡å‹ç‰ˆæœ¬ã€‚ | Select the available model version."
                    }
                ),
                "quantization": (
                    [
                        "ğŸ‘ 4-bit (VRAM-friendly)",
                        "âš–ï¸ 8-bit (Balanced Precision)",
                        "ğŸš« None (Original Precision)"
                    ],
                    {
                        "default": "ğŸ‘ 4-bit (VRAM-friendly)",
                        "tooltip": "é€‰æ‹©é‡åŒ–çº§åˆ«:\nâœ… 4-bit: æ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚\nâš–ï¸ 8-bit: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ã€‚\nğŸš« None: ä½¿ç”¨åŸå§‹ç²¾åº¦ï¼ˆéœ€è¦é«˜ç«¯GPUï¼‰ã€‚ \n "
                                   "Select the quantization level:\nâœ… 4-bit: Significantly reduces VRAM usage.\nâš–ï¸ 8-bit: Balances precision and performance.\nğŸš« None: Uses original precision (requires high-end GPU)."
                    }
                ),
                "enable_thinking": (
                    "BOOLEAN",
                    {
                        "default": False,
                        "tooltip": "å¯ç”¨æˆ–ç¦ç”¨æ€è€ƒæ¨¡å¼ã€‚æ€è€ƒæ¨¡å¼é€‚ç”¨äºå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œéæ€è€ƒæ¨¡å¼é€‚ç”¨äºé«˜æ•ˆå¯¹è¯ã€‚ | "
                                   "Enable or disable thinking mode. Thinking mode is suitable for complex reasoning tasks, while non-thinking mode is optimized for efficient conversations."
                    }
                ),
                "prompt": ("STRING", {
                    "default": "This is the prompt text used for generating images with Fulx: \"In the style of GHIBSKY, a cyberpunk panda holding a neon sign that reads: 'Designed by SXQBW'\". Please optimize, supplement and improve the prompt text according to its content, and make the generated image effect the best.",
                    "multiline": True,
                    "tooltip": "è¾“å…¥æç¤ºæ–‡æœ¬ | Enter the prompt text"
                }),
                "max_new_tokens": ("INT", {
                    "default": 1024,
                    "min": 64,
                    "max": 2048,
                    "step": 16,
                    "display": "slider",
                    "tooltip": "æ§åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•° | Control the maximum number of tokens to generate"
                }),
                "clear_history": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦æ¸…é™¤å¯¹è¯å†å² | Whether to clear the conversation history"
                }),
                "unload_after_generation": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ç”Ÿæˆåæ˜¯å¦å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾èµ„æºã€‚å¯ç”¨æ­¤é€‰é¡¹å¯å‡å°‘å†…å­˜å ç”¨ï¼Œä½†ä¼šå¢åŠ ä¸‹æ¬¡ä½¿ç”¨æ—¶çš„åŠ è½½æ—¶é—´ã€‚ | "
                               "Whether to unload the model after generation to free up resources. Enabling this option reduces memory usage but increases load time for subsequent uses."
                })
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "process"
    CATEGORY = "ğŸ¼Qwen"

    def process(self, model_name, quantization, enable_thinking, prompt, max_new_tokens, clear_history, unload_after_generation):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…é™¤å†å²
        if clear_history:
            self.chat_history = []
            print("å¯¹è¯å†å²å·²æ¸…é™¤ | Conversation history cleared")
            
        # ä½¿ç”¨ç±»çº§åˆ«çš„å¤„ç†å™¨å®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
        if QwenMultiTurnConversation.processor is None:
            QwenMultiTurnConversation.processor = QwenTextProcessor()

        # ç¡®ä¿å¤„ç†å™¨å·²åŠ è½½æ¨¡å‹
        if QwenMultiTurnConversation.processor.model is None:
            QwenMultiTurnConversation.processor.load_model(model_name, quantization, enable_thinking)   
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
        self.chat_history.append({"role": "user", "content": prompt})
        
        # ç”Ÿæˆå›å¤ï¼Œä¼ é€’å®Œæ•´çš„å¯¹è¯å†å²
        generated_text = QwenMultiTurnConversation.processor.generate_text(
            model_name=model_name,
            quantization=quantization,
            prompt=prompt,
            max_tokens=max_new_tokens,
            messages=self.chat_history,
            enable_thinking=enable_thinking,
            unload_after_generation=unload_after_generation
        )
        
        if isinstance(generated_text, list):
            # å¦‚æœè¿”å›çš„æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæå–å†…å®¹
            generated_text = "".join([msg.get("content", "") for msg in generated_text if msg.get("role") == "assistant"])
        
        # æ›¿æ¢æ€è€ƒæ ‡ç­¾
        generated_text = replace_thinking_tags(generated_text)

        if not enable_thinking:
            generated_text = remove_thinking_tags(generated_text)
        
        # å°†Markdownè½¬æ¢ä¸ºçº¯æ–‡æœ¬
        plain_text = markdown_to_plaintext(generated_text)
        
        # æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
        self.chat_history.append({"role": "assistant", "content": plain_text})
        
        # æ ¼å¼åŒ–å¹¶è¿”å›å®Œæ•´çš„å¯¹è¯å†å²
        formatted_history = self.format_chat_history()
        return (formatted_history,)
    
    def format_chat_history(self):
        """æ ¼å¼åŒ–å¯¹è¯å†å²ä»¥ä¾¿æ˜¾ç¤º"""
        formatted_history = []
        for message in self.chat_history:
            role = message["role"].upper()
            content = message["content"]
            
            # ç¡®ä¿contentæ˜¯å­—ç¬¦ä¸²ç±»å‹
            if isinstance(content, list):
                content = " ".join(map(str, content))
            elif not isinstance(content, str):
                content = str(content)
            
            # æ·»åŠ åˆ†éš”ç¬¦å’Œè§’è‰²æ ‡è¯†
            formatted_history.append(f"[{role}]")
            formatted_history.append(content)
            formatted_history.append("-" * 60)  # åˆ†éš”çº¿
        
        return "\n".join(formatted_history)


class QwenSingleTurnGeneration:
    """Qwen3å•è½®ç”ŸæˆèŠ‚ç‚¹ | Qwen3 Single-turn Generation Node"""
    # ç±»çº§åˆ«å˜é‡ï¼Œç”¨äºç¼“å­˜å¤„ç†å™¨å®ä¾‹
    processor = None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (
                   list(MODEL_REGISTRY.keys()),  # åŠ¨æ€ç”Ÿæˆæ¨¡å‹é€‰é¡¹ | Dynamically generate model options
                    {
                        "default": next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                                       list(MODEL_REGISTRY.keys())[0]),
                        "tooltip": "é€‰æ‹©å¯ç”¨çš„æ¨¡å‹ç‰ˆæœ¬ã€‚ | Select the available model version."
                    }
                ),
                "quantization": (
                    [
                        "ğŸ‘ 4-bit (VRAM-friendly)",
                        "âš–ï¸ 8-bit (Balanced Precision)",
                        "ğŸš« None (Original Precision)"
                    ],
                    {
                        "default": "ğŸ‘ 4-bit (VRAM-friendly)",
                        "tooltip": "é€‰æ‹©é‡åŒ–çº§åˆ«:\nâœ… 4-bit: æ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚\nâš–ï¸ 8-bit: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ã€‚\nğŸš« None: ä½¿ç”¨åŸå§‹ç²¾åº¦ï¼ˆéœ€è¦é«˜ç«¯GPUï¼‰ã€‚ \n "
                                   "Select the quantization level:\nâœ… 4-bit: Significantly reduces VRAM usage.\nâš–ï¸ 8-bit: Balances precision and performance.\nğŸš« None: Uses original precision (requires high-end GPU)."
                    }
                ),
                "enable_thinking": (
                    "BOOLEAN",
                    {
                        "default": False,
                        "tooltip": "å¯ç”¨æˆ–ç¦ç”¨æ€è€ƒæ¨¡å¼ã€‚æ€è€ƒæ¨¡å¼é€‚ç”¨äºå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œéæ€è€ƒæ¨¡å¼é€‚ç”¨äºé«˜æ•ˆå¯¹è¯ã€‚ | "
                                   "Enable or disable thinking mode. Thinking mode is suitable for complex reasoning tasks, while non-thinking mode is optimized for efficient conversations."
                    }
                ),
                "prompt": ("STRING", {
                    "default": "This is the prompt text used for generating images with Fulx: \"In the style of GHIBSKY, a cyberpunk panda holding a neon sign that reads: 'Designed by SXQBW'\". Please optimize, supplement and improve the prompt text according to its content, and make the generated image effect the best.Just provide the best answer content.",
                    "multiline": True,
                    "tooltip": "è¾“å…¥æç¤ºæ–‡æœ¬ | Enter the prompt text"
                }),
                "max_new_tokens": ("INT", {
                    "default": 2048,
                    "min": 64,
                    "max": 6144,
                    "step": 32,
                    "display": "slider",
                    "tooltip": "æ§åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•° | Control the maximum number of tokens to generate"
                }),
                "unload_after_generation": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ç”Ÿæˆåæ˜¯å¦å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾èµ„æºã€‚å¯ç”¨æ­¤é€‰é¡¹å¯å‡å°‘å†…å­˜å ç”¨ï¼Œä½†ä¼šå¢åŠ ä¸‹æ¬¡ä½¿ç”¨æ—¶çš„åŠ è½½æ—¶é—´ã€‚ | "
                               "Whether to unload the model after generation to free up resources. Enabling this option reduces memory usage but increases load time for subsequent uses."
                })
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "process"
    CATEGORY = "ğŸ¼Qwen"

    def process(self, model_name, quantization, enable_thinking, prompt, max_new_tokens, unload_after_generation):
        # ä½¿ç”¨ç±»çº§åˆ«çš„å¤„ç†å™¨å®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
        if QwenSingleTurnGeneration.processor is None:
            QwenSingleTurnGeneration.processor = QwenTextProcessor()

        # ç¡®ä¿å¤„ç†å™¨å·²åŠ è½½æ¨¡å‹
        if QwenSingleTurnGeneration.processor.model is None:
            QwenSingleTurnGeneration.processor.load_model(model_name, quantization, enable_thinking)              
        
        # ç”Ÿæˆå›å¤
        generated_text = QwenSingleTurnGeneration.processor.generate_text(
            model_name=model_name,
            quantization=quantization,
            prompt=prompt,
            max_tokens=max_new_tokens,
            enable_thinking=enable_thinking,
            unload_after_generation=unload_after_generation
        )        

        if isinstance(generated_text, list):
            # å¦‚æœè¿”å›çš„æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæå–å†…å®¹
            generated_text = "".join([msg.get("content", "") for msg in generated_text if msg.get("role") == "assistant"])
        
        # æ›¿æ¢æ€è€ƒæ ‡ç­¾
        generated_text = replace_thinking_tags(generated_text)

        if not enable_thinking:
            generated_text = remove_thinking_tags(generated_text)
        
        # å°†Markdownè½¬æ¢ä¸ºçº¯æ–‡æœ¬
        plain_text = markdown_to_plaintext(generated_text)
        
        return (plain_text,)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "QwenMultiTurnConversation": QwenMultiTurnConversation,
    "QwenSingleTurnGeneration": QwenSingleTurnGeneration,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenMultiTurnConversation": "Qwen Conversation ğŸ¼",
    "QwenSingleTurnGeneration": "Qwen Generation ğŸ¼",
}